{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.17 🚀 Python-3.12.3 torch-2.3.0 CPU (Apple M2 Pro)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/treyshanks/data_science/Court_Vision/agg_lebron/valid/labels.cache... 239 images, 0 backgrounds, 0 corrupt: 100%|██████████| 239/239 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 6, len(boxes) = 239. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 15/15 [00:42<00:00,  2.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        239        239      0.935        0.9      0.947       0.57\n",
      "Speed: 0.6ms preprocess, 172.8ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Saving /Users/treyshanks/data_science/Court_Vision/data/outputs/weights/predictions.json...\n",
      "Results saved to \u001b[1m/Users/treyshanks/data_science/Court_Vision/data/outputs/weights\u001b[0m\n",
      "Ultralytics YOLOv8.2.17 🚀 Python-3.12.3 torch-2.3.0 CPU (Apple M2 Pro)\n",
      "Model summary (fused): 218 layers, 25840339 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/treyshanks/data_science/Court_Vision/agg_lebron/valid/labels.cache... 239 images, 0 backgrounds, 0 corrupt: 100%|██████████| 239/239 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Box and segment counts should be equal, but got len(segments) = 6, len(boxes) = 239. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 15/15 [02:59<00:00, 11.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        239        239      0.897      0.908      0.935      0.535\n",
      "Speed: 0.5ms preprocess, 748.5ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Saving /Users/treyshanks/data_science/Court_Vision/data/outputs/notebooks/predictions.json...\n",
      "Results saved to \u001b[1m/Users/treyshanks/data_science/Court_Vision/data/outputs/notebooks\u001b[0m\n",
      "Metrics for model 1:\n",
      "mAP50-95: 0.5703\n",
      "mAP50: 0.9465\n",
      "mAP75: 0.6345\n",
      "Class-wise mAPs: [    0.57028]\n",
      "Metrics for model 2:\n",
      "mAP50-95: 0.5351\n",
      "mAP50: 0.9354\n",
      "mAP75: 0.5787\n",
      "Class-wise mAPs: [    0.53507]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Paths to your model weight files and data.yaml file\n",
    "model_paths = [\n",
    "    '/Users/treyshanks/data_science/Court_Vision/lebron_collab/weights/best.pt',\n",
    "    '/Users/treyshanks/data_science/Court_Vision/notebooks/lebron_aug_best.pt'\n",
    "]\n",
    "data_yaml_path = '/Users/treyshanks/data_science/Court_Vision/agg_lebron/data.yaml'\n",
    "\n",
    "# List to store metrics for each model\n",
    "metrics_list = []\n",
    "\n",
    "# Validate each model\n",
    "for model_path in model_paths:\n",
    "    # Load the model\n",
    "    model = YOLO(model_path)\n",
    "    model.model.to('mps')\n",
    "    model.data = data_yaml_path  # Ensure the model uses the correct dataset configuration\n",
    "    \n",
    "    # Validate the model\n",
    "    metrics = model.val(\n",
    "        data=data_yaml_path,\n",
    "        device='mps',\n",
    "        save_json=True,\n",
    "        project=\"/Users/treyshanks/data_science/Court_Vision/data/outputs\",\n",
    "        name=f\"{model_path.split('/')[-1]}\"\n",
    "        ) \n",
    "    \n",
    "    # Store the metrics\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "# Compare the results\n",
    "for i, metrics in enumerate(metrics_list):\n",
    "    print(f\"Metrics for model {i+1}:\")\n",
    "    print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
    "    print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
    "    print(f\"mAP75: {metrics.box.map75:.4f}\")\n",
    "    print(f\"Class-wise mAPs: {metrics.box.maps}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
